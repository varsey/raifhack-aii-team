{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00fa881",
   "metadata": {},
   "source": [
    "## 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "77fc8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d15644",
   "metadata": {},
   "source": [
    "## 1. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "63720e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData:\n",
    "    \n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.categorical = None\n",
    "        self.numerical = None\n",
    "        self.target = None\n",
    "        \n",
    "        \n",
    "    def handle_na(self, columns: list, type_list: list):\n",
    "        \n",
    "        for col, tp in zip(columns, type_list):\n",
    "            for df in [self.train, self.test]:\n",
    "                if tp == 'cat':\n",
    "                    df[col] = df[col].fillna('missing')\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(df[col].mode().values[0])\n",
    "                    df[col] = df[col].replace(r'\\.0$', '', regex=True)\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "    \n",
    "    def replace_cat(self, columns: list, type_list: list, replace_cat_dict: dict):\n",
    "        \n",
    "        for col, tp in zip(columns, type_list):\n",
    "            for df in [self.train, self.test]:\n",
    "                if tp == 'cat':\n",
    "                    df[col] = df[col].replace(replace_cat_dict)\n",
    "                else:\n",
    "                    df[col] = df[col].replace(replace_cat_dict)\n",
    "                    df[col] = df[col].replace(r'\\.0$', '', regex=True)\n",
    "    \n",
    "    \n",
    "    def parse_date(self, date_column):\n",
    "        \n",
    "        for df in [self.train, self.test]:\n",
    "            df[date_column] = pd.to_datetime(df[date_column])\n",
    "            df['month'] = df[date_column].dt.month\n",
    "            df['dayofmonth'] = df[date_column].dt.day\n",
    "            df['dayofweek'] = df[date_column].dt.dayofweek\n",
    "            df['dayofyear'] = df[date_column].dt.dayofyear\n",
    "\n",
    "    def encode_and_scale(self, \n",
    "                         ordinal_columns: list = None, \n",
    "                         scale_columns: list = None,\n",
    "                         log_columns: list = None,\n",
    "                         one_hot_columns: list = None,\n",
    "                         scale='standard'):\n",
    "        \n",
    "        get_train = []\n",
    "        get_test = []\n",
    "        \n",
    "        \n",
    "        if ordinal_columns:\n",
    "            # Encode ordinal features.\n",
    "            ord_ = OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                                 unknown_value=-1)\n",
    "            ord_.fit(self.train[ordinal_columns])\n",
    "\n",
    "            ord_train = pd.DataFrame(ord_.transform(\n",
    "                self.train[ordinal_columns]), columns=ordinal_columns)\n",
    "            ord_test = pd.DataFrame(ord_.transform(\n",
    "                self.test[ordinal_columns]), columns=ordinal_columns)\n",
    "            \n",
    "            get_train.append(ord_train)\n",
    "            get_test.append(ord_test)\n",
    "        \n",
    "        if one_hot_columns:\n",
    "            # Encode categorical features.\n",
    "            ohe_ = OneHotEncoder()\n",
    "            ohe_.fit(self.train[one_hot_columns])\n",
    "\n",
    "            ohe_train = pd.DataFrame(ohe_.transform(self.train[one_hot_columns]).toarray())\n",
    "            ohe_test = pd.DataFrame(ohe_.transform(self.test[one_hot_columns]).toarray())\n",
    "            \n",
    "            get_train.append(ohe_train)\n",
    "            get_test.append(ohe_test)\n",
    "            \n",
    "        if scale_columns:\n",
    "            # Rescale numerical features.\n",
    "            if scale == 'standard':\n",
    "                sc = StandardScaler()\n",
    "            else:\n",
    "                sc = MinMaxScaler(feature_range=(0, 1))\n",
    "            scale_train = pd.DataFrame(sc.fit_transform(self.train[scale_columns]),\n",
    "                                       columns=scale_columns)\n",
    "            scale_test = pd.DataFrame(sc.fit_transform(self.test[scale_columns]),\n",
    "                                      columns=scale_columns)\n",
    "            \n",
    "            get_train.append(scale_train)\n",
    "            get_test.append(scale_test)\n",
    "            \n",
    "        if log_columns:\n",
    "            log_train = np.log(self.train)\n",
    "            log_test = np.log(self.test)\n",
    "            \n",
    "            get_train.append(log_train)\n",
    "            get_test.append(log_test)\n",
    "        \n",
    "        \n",
    "        # Combine DataFrames.\n",
    "        self.train = reduce(lambda x, y: pd.concat([x, y], axis=1), get_train)\n",
    "        \n",
    "        self.test = reduce(lambda x, y: pd.concat([x, y], axis=1), get_test)\n",
    "        \n",
    "    \n",
    "    def column_types(self, target=None, \n",
    "                     extend_cat=None, extend_num=None,\n",
    "                     remove_cat=None, remove_num=None):\n",
    "        # Numerical features.\n",
    "        self.numerical = [f for f in self.train.columns if \n",
    "                          self.train.dtypes[f] != 'object']\n",
    "        # Categorical features.\n",
    "        self.categorical = [f for f in self.train.columns if\n",
    "                            self.train.dtypes[f] == 'object']\n",
    "        # Remove target\n",
    "        self.target = self.train[[target]]\n",
    "        self.numerical.remove(target)\n",
    "\n",
    "        # Extend list of column names.\n",
    "        if extend_cat:\n",
    "            self.categorical.extend(extend_cat)\n",
    "        if extend_num:\n",
    "            self.numerical.extend(extend_num)\n",
    "        \n",
    "        # Delete specified column names form list.\n",
    "        if remove_cat:\n",
    "            self.categorical = [f for f in self.categorical if\n",
    "                                f not in remove_cat]\n",
    "        if remove_num:\n",
    "            self.numerical = [f for f in self.numerical if \n",
    "                              f not in remove_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a3c21",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6f78c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dicatioanry for the 'floor' column.\n",
    "with open('data/data.json') as json_file:\n",
    "    floor_dict = json.load(json_file)\n",
    "# Load data.\n",
    "train = pd.read_csv('data/train.csv', low_memory=False)\n",
    "test = pd.read_csv('data/test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3ea000b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluns with mmissing values.\n",
    "na_cols = train.isnull().sum()[train.isnull().sum()>0].index.to_list()\n",
    "# Types of columns containing mmissing values.\n",
    "type_list = ['num', 'num', 'num', 'num', 'num', 'num', 'num', 'num', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0b248",
   "metadata": {},
   "source": [
    "Apply data processing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c249f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data instance.\n",
    "data = ProcessData(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6695925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'floor' column data.\n",
    "data.replace_cat(columns=['floor'], replace_cat_dict=floor_dict, type_list=['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e4ab7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values.\n",
    "data.handle_na(na_cols, type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1b23c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date-related columns.\n",
    "data.parse_date('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "64eece07",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'per_square_meter_price'\n",
    "\n",
    "extend_cat = ['realty_type']\n",
    "remove_cat = ['id', 'date']\n",
    "\n",
    "remove_num = ['date', 'realty_type']\n",
    "\n",
    "# Get column types.\n",
    "data.column_types(target=target, extend_cat=extend_cat,\n",
    "                  remove_cat=remove_cat, remove_num=remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fc59add4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'floor', 'id', 'lat', 'lng', 'osm_amenity_points_in_0.001',\n",
       "       'osm_amenity_points_in_0.005', 'osm_amenity_points_in_0.0075',\n",
       "       'osm_amenity_points_in_0.01', 'osm_building_points_in_0.001',\n",
       "       'osm_building_points_in_0.005', 'osm_building_points_in_0.0075',\n",
       "       'osm_building_points_in_0.01', 'osm_catering_points_in_0.001',\n",
       "       'osm_catering_points_in_0.005', 'osm_catering_points_in_0.0075',\n",
       "       'osm_catering_points_in_0.01', 'osm_city_closest_dist',\n",
       "       'osm_city_nearest_name', 'osm_city_nearest_population',\n",
       "       'osm_crossing_closest_dist', 'osm_crossing_points_in_0.001',\n",
       "       'osm_crossing_points_in_0.005', 'osm_crossing_points_in_0.0075',\n",
       "       'osm_crossing_points_in_0.01', 'osm_culture_points_in_0.001',\n",
       "       'osm_culture_points_in_0.005', 'osm_culture_points_in_0.0075',\n",
       "       'osm_culture_points_in_0.01', 'osm_finance_points_in_0.001',\n",
       "       'osm_finance_points_in_0.005', 'osm_finance_points_in_0.0075',\n",
       "       'osm_finance_points_in_0.01', 'osm_healthcare_points_in_0.005',\n",
       "       'osm_healthcare_points_in_0.0075', 'osm_healthcare_points_in_0.01',\n",
       "       'osm_historic_points_in_0.005', 'osm_historic_points_in_0.0075',\n",
       "       'osm_historic_points_in_0.01', 'osm_hotels_points_in_0.005',\n",
       "       'osm_hotels_points_in_0.0075', 'osm_hotels_points_in_0.01',\n",
       "       'osm_leisure_points_in_0.005', 'osm_leisure_points_in_0.0075',\n",
       "       'osm_leisure_points_in_0.01', 'osm_offices_points_in_0.001',\n",
       "       'osm_offices_points_in_0.005', 'osm_offices_points_in_0.0075',\n",
       "       'osm_offices_points_in_0.01', 'osm_shops_points_in_0.001',\n",
       "       'osm_shops_points_in_0.005', 'osm_shops_points_in_0.0075',\n",
       "       'osm_shops_points_in_0.01', 'osm_subway_closest_dist',\n",
       "       'osm_train_stop_closest_dist', 'osm_train_stop_points_in_0.005',\n",
       "       'osm_train_stop_points_in_0.0075', 'osm_train_stop_points_in_0.01',\n",
       "       'osm_transport_stop_closest_dist', 'osm_transport_stop_points_in_0.005',\n",
       "       'osm_transport_stop_points_in_0.0075',\n",
       "       'osm_transport_stop_points_in_0.01', 'per_square_meter_price',\n",
       "       'reform_count_of_houses_1000', 'reform_count_of_houses_500',\n",
       "       'reform_house_population_1000', 'reform_house_population_500',\n",
       "       'reform_mean_floor_count_1000', 'reform_mean_floor_count_500',\n",
       "       'reform_mean_year_building_1000', 'reform_mean_year_building_500',\n",
       "       'region', 'total_square', 'street', 'date', 'realty_type', 'price_type',\n",
       "       'month', 'dayofmonth', 'dayofweek', 'dayofyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008d6cc",
   "metadata": {},
   "source": [
    "1. ordinal_columns - `OrdinalEncoder`, \n",
    "2. scale_columns - если `scale='standard'`, то `StandardScaler`, в противном случае `MinMaxScaler`\n",
    "3. log_columns - `np.log()`,\n",
    "4. one_hot_column - `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "86ed1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and rescale data\n",
    "data.encode_and_scale(one_hot_columns=['region'],\n",
    "                      ordinal_columns=[x for x in data.categorical if x!='region'],\n",
    "                      scale_columns=data.numerical,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a5c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f4fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "706346e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_save = pd.concat([data.train, data.target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a609a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_save.to_csv('data_cleaned_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b3952480",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_save.to_csv('train_cleaned_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2e024dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.to_csv('test_cleaned_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4f14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9c643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
